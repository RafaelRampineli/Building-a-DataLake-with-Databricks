{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83f1d37-a186-45e2-bf16-4c17bf1078bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: [FileInfo(path='dbfs:/FileStore/tables/datalake/csv/categories.csv', name='categories.csv', size=432, modificationTime=1737052601000),\n FileInfo(path='dbfs:/FileStore/tables/datalake/csv/customers.csv', name='customers.csv', size=12345, modificationTime=1737052602000),\n FileInfo(path='dbfs:/FileStore/tables/datalake/csv/employees.csv', name='employees.csv', size=4593, modificationTime=1737052602000),\n FileInfo(path='dbfs:/FileStore/tables/datalake/csv/orderdetails.csv', name='orderdetails.csv', size=46804, modificationTime=1737052599000),\n FileInfo(path='dbfs:/FileStore/tables/datalake/csv/orders.csv', name='orders.csv', size=136684, modificationTime=1737052599000),\n FileInfo(path='dbfs:/FileStore/tables/datalake/csv/products.csv', name='products.csv', size=4773, modificationTime=1737052600000),\n FileInfo(path='dbfs:/FileStore/tables/datalake/csv/shippers.csv', name='shippers.csv', size=148, modificationTime=1737052600000),\n FileInfo(path='dbfs:/FileStore/tables/datalake/csv/suppliers.csv', name='suppliers.csv', size=4642, modificationTime=1737052601000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"dbfs:/FileStore/tables/datalake/csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f00897-3cb8-4757-8961-01381696f1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Csv Files\n",
    "files = dbutils.fs.ls(\"dbfs:/FileStore/tables/datalake/csv\")\n",
    "# Directory to save our Datalake Delta\n",
    "delta_path = \"dbfs:/FileStore/tables/datalake/delta\"\n",
    "datalake_extension = \"delta\"\n",
    "# Extract file names\n",
    "for file in files:\n",
    "    file_path = file.path\n",
    "    dataframe = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(file_path)\n",
    "    \n",
    "    # Getting the name of file to concat\n",
    "    if file.name.endswith(\".csv\"):\n",
    "        file_name_without_extension = file.name[:-4]  # Remove the last 4 characters (\".csv\")\n",
    "    \n",
    "    # Write to Delta\n",
    "    dataframe.write.format(\"delta\").mode(\"overwrite\").save(f\"{delta_path}/{file_name_without_extension}.{datalake_extension}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3bb986-0970-4aeb-b98c-bf6d9c12217b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading some Data at Delta Tables\n",
    "df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/categories.delta\")\n",
    "\n",
    "# Adding a new row to Delta\n",
    "new = spark.createDataFrame([(9, \"coffee\", \"Moka pot, Aeropress, cappuccino\")], df.schema)\n",
    "new.write.format(\"delta\").mode(\"append\").save(\"dbfs:/FileStore/tables/datalake/delta/categories.delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e574ccd-d60e-439e-88ab-c5c8157251d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#upsert \n",
    "from delta.tables import *\n",
    "\n",
    "# Loading tables as DeltaTable again\n",
    "deltaTable_orders = DeltaTable.forPath(spark, \"dbfs:/FileStore/tables/datalake/delta/orders.delta\")\n",
    "deltaTable_order_details = DeltaTable.forPath(spark, \"dbfs:/FileStore/tables/datalake/delta/orderdetails.delta\")\n",
    "\n",
    "# New values to be inserted\n",
    "new_order = spark.createDataFrame([(11078, \"ALFKI\", 1, \"2023-08-01\")], [\"OrderID\", \"CustomerID\", \"EmployeeID\", \"OrderDate\"])\n",
    "new_order_details = spark.createDataFrame([(11078, 1, 18, 3)], [\"OrderID\", \"ProductID\", \"UnitPrice\", \"Quantity\"])\n",
    "\n",
    "deltaTable_orders.alias(\"orders\").merge(\n",
    "    new_order.alias(\"newOrder\"),\n",
    "    \"orders.OrderID = newOrder.OrderID\")\\\n",
    "    .whenMatchedUpdate(set = {\"CustomerID\" : \"newOrder.CustomerID\", \"EmployeeID\" : \"newOrder.EmployeeID\", \"OrderDate\" : \"newOrder.OrderDate\"})\\\n",
    "    .whenNotMatchedInsert(values = {\"OrderID\" : \"newOrder.OrderID\", \"CustomerID\" : \"newOrder.CustomerID\", \"EmployeeID\" : \"newOrder.EmployeeID\", \"OrderDate\" : \"newOrder.OrderDate\"})\\\n",
    "    .execute()\n",
    "\n",
    "deltaTable_order_details.alias(\"order_details\").merge(\n",
    "    new_order_details.alias(\"newOrderDetails\"),\n",
    "    \"order_details.OrderID = newOrderDetails.OrderID AND order_details.ProductID = newOrderDetails.ProductID\")\\\n",
    "    .whenMatchedUpdate(set = {\"UnitPrice\" : \"newOrderDetails.UnitPrice\", \"Quantity\" : \"newOrderDetails.Quantity\"})\\\n",
    "    .whenNotMatchedInsert(values = {\"OrderID\" : \"newOrderDetails.OrderID\", \"ProductID\" : \"newOrderDetails.ProductID\", \"UnitPrice\" : \"newOrderDetails.UnitPrice\", \"Quantity\" : \"newOrderDetails.Quantity\"})\\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399b08e4-2d21-4a9e-8091-c27c594eca62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------------+------------+-----------+-------+-------+--------+-----------+--------+----------+--------------+-----------+\n|OrderID|CustomerID|EmployeeID|          OrderDate|RequiredDate|ShippedDate|ShipVia|Freight|ShipName|ShipAddress|ShipCity|ShipRegion|ShipPostalCode|ShipCountry|\n+-------+----------+----------+-------------------+------------+-----------+-------+-------+--------+-----------+--------+----------+--------------+-----------+\n|  11078|     ALFKI|         1|2023-08-01 00:00:00|        null|       null|   null|   null|    null|       null|    null|      null|          null|       null|\n+-------+----------+----------+-------------------+------------+-----------+-------+-------+--------+-----------+--------+----------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Reading some data using a basic filter\n",
    "df_orders = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/orders.delta\").filter(\"OrderID == 11078\")\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75da1d6f-5214-429b-aecb-8bdbb6e57acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+--------+--------+\n|OrderID|ProductID|UnitPrice|Quantity|Discount|\n+-------+---------+---------+--------+--------+\n|  11078|        1|     18.0|       3|    null|\n+-------+---------+---------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Reading some data using a basic filter\n",
    "df_order_details = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/orderdetails.delta\").filter(\"OrderID == 11078\").filter(\"ProductID == 1\")\n",
    "df_order_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea57b7f-3cec-498b-b63b-85117a2d4013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating some temporary views from Dataframe.\n",
    "# The views are only accessible within the session it is created in, enabling you to use SQL queries to manipulate the data as if it were a table.\n",
    "\n",
    "# Directory Datalake Delta\n",
    "delta_file = \"dbfs:/FileStore/tables/datalake/delta/\"\n",
    "delta_path =  dbutils.fs.ls(delta_file)\n",
    "datalake_extension = \"delta\"\n",
    "# Extract file names\n",
    "for file in delta_path:    \n",
    "    df = spark.read.format(\"delta\").load(file.path[:-1]) # Remove the last character (\"/\")\n",
    "    df.createOrReplaceTempView(file.name[:-7]) # Remove the last 7 characters (\".delta/\")\n",
    "    \n",
    "join_query = \"\"\"\n",
    "SELECT orderdetails .OrderID AS OrderID, orderdetails .Quantity , orderdetails .UnitPrice as UnitPrice,\n",
    "products.ProductID as ProductID,  products.ProductName as Product, suppliers.CompanyName AS Suppliers,  \n",
    "employees.LastName as Employee, orders.OrderDate as Date, customers.CompanyName as Customer\n",
    "FROM orders\n",
    "JOIN orderdetails  ON orders.OrderID = orderdetails .OrderID\n",
    "JOIN products ON orderdetails .ProductID = products.ProductID\n",
    "JOIN categories ON products.CategoryID = categories.CategoryID\n",
    "JOIN suppliers ON products.SupplierID = suppliers.SupplierID\n",
    "JOIN employees ON orders.EmployeeID = employees.EmployeeID\n",
    "JOIN shippers ON orders.ShipVia = shippers.ShipperID\n",
    "JOIN customers ON orders.CustomerID = customers.CustomerID\n",
    "\"\"\"\n",
    "\n",
    "df_result = spark.sql(join_query)\n",
    "\n",
    "# Write result in a new Delta Table\n",
    "df_result.write.format(\"delta\").mode(\"overwrite\").save(f\"{delta_file}join.{datalake_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec971426-1756-4071-9fef-9db0b0509970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+---------+--------------+--------------------+--------+-------------------+--------------------+\n|OrderID|Quantity|UnitPrice|ProductID|       Product|           Suppliers|Employee|               Date|            Customer|\n+-------+--------+---------+---------+--------------+--------------------+--------+-------------------+--------------------+\n|  10248|      12|     14.0|       11|Queso Cabrales|Cooperativa de Qu...|Buchanan|2020-07-04 00:00:00|Vins et alcools C...|\n+-------+--------+---------+---------+--------------+--------------------+--------+-------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Reading our Delta using SQL \n",
    "df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/join.delta\")\n",
    "df.createOrReplaceTempView(\"OrdersJoin\")\n",
    "\n",
    "results = spark.sql(\"SELECT * FROM OrdersJoin WHERE OrderID = 10248 AND ProductID =11 \")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2341d7f-64ad-4eb7-a960-4a308869cd98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Clean up \n",
    "dbutils.fs.rm(\"dbfs:/FileStore/tables/datalake/delta/\", recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DeltaLake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
